{
    # General
    {
        "epoches": 1, 
        "val_step": 1, 
        "checkpoint_step": 1, 
        "clip_max_norm": -1
    },
    # Optimizer
    {
        "optimizer": "Adam", 
        "learning_rate": 0.0002, 
        "weight_decay": 0.0001, 
        "clip_gradient_norm": False, 
        "clip_gradient_norm_value": 0.1, 
        "beta1": 0.9, 
        "beta2": 0.999
    }

    # Scheduler
    {
        "type": "MultiStepLR", 
         "enable_warmup": True, 
        "warmup_iterations": 25
    }

    # Scheduler class
    {
        "type": "MultiStepLR",
        "milestones": [16, 22], 
        "gamma": 0.1
    }

    # All
    {
        "epoches": 20, 
        "val_step": 1, 
        "checkpoint_step": 1, 
        "save_optimizer": False, 
        "save_ema": False, 
        "train_dataloader": 
            {
                "dataset": {...}, 
                "shuffle": True, 
                "batch_size": 4, 
                "num_workers": 4
            }, 
        "val_dataloader": 
            {
                "dataset": {...}, 
                "shuffle": False, 
                "batch_size": 8, 
                "num_workers": 8
            }, 
        "optimizer": 
            {
                "type": "AdamW", 
                "lr": 0.0002, 
                "betas": [...], 
                "weight_decay": 0.0001
            }, 
        "clip_max_norm": -1, 
        "lr_scheduler": 
            {
                "type": "MultiStepLR", 
                "milestones": [...], 
                "gamma": 0.1
            }, 
        "use_ema": True, 
        "ema": 
            {
                "type": "ModelEMA", 
                "decay": 0.9999, 
                "warmups": 2000
            }, 
        "HybridEncoder": {"eval_spatial_size": [...]}, 
        "RTDETRTransformer": {"eval_spatial_size": [...]}, 
        "lr_warmup": 
            {
                "type": "LinearLR", 
                "total_iters": 25, 
                "start_factor": 0.001, 
                "end_factor": 1.0
            }
        }
}